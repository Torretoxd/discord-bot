import asyncio
import os
import requests

HF_API_KEY = os.getenv("HF_API_KEY")

# Free, lightweight model (good for short answers)
MODEL_URL = "https://api-inference.huggingface.co/models/google/flan-t5-base"

headers = {
    "Authorization": f"Bearer {HF_API_KEY}"
}

user_cooldowns = {}
COOLDOWN = 30  # seconds


async def handle_ai_message(message):
    user_id = message.author.id
    now = asyncio.get_event_loop().time()

    # Cooldown check
    if user_id in user_cooldowns and now - user_cooldowns[user_id] < COOLDOWN:
        await message.channel.send("Slow down ðŸ˜„ give me a moment.")
        return

    user_cooldowns[user_id] = now

    # Extract question
    question = message.content.lower().replace("dowi", "").replace(",", "").strip()
    if not question:
        await message.channel.send("Yes? What do you want to know?")
        return

    payload = {
        "inputs": f"Answer briefly and clearly: {question}",
        "parameters": {
            "max_new_tokens": 200
        }
    }

    try:
        response = requests.post(MODEL_URL, headers=headers, json=payload, timeout=30)
        data = response.json()

    # Hugging Face can return different formats
    if isinstance(data, list) and len(data) > 0:
        answer = data[0].get("generated_text")
        if answer:
            await message.channel.send(answer.strip())
            return

    # Model loading / error message
    if isinstance(data, dict) and "error" in data:
        await message.channel.send(
            "ðŸ§  Iâ€™m warming upâ€¦ try again in a few seconds!"
        )
        print("HF MODEL ERROR:", data)
        return

    # Fallback
    await message.channel.send("I couldnâ€™t think of a good answer ðŸ¤”")
    print("HF RAW RESPONSE:", data)

    except Exception as e:
        print("HF AI ERROR:", repr(e))
        await message.channel.send("My brain lagged a bit ðŸ˜µ try again soon.")
